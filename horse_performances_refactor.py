import logging
import db_handler_persistent as dbh

import numpy as np
import pandas as pd
import datetime
import re
from progress.bar import Bar

# Import configuration settings
from horse_performances_consolidated_table_structure import CONSOLIDATED_TABLE_STRUCTURE
from horse_performances_additional_fields import ADDITIONAL_FIELDS
from horse_performances_table_to_index_mappings import TABLE_TO_INDEX_MAPPINGS
from horse_performances_distances_to_process import DISTANCES_TO_PROCESS
from horse_performances_position_distance_mappings import POSITION_DISTANCE_MAPPINGS
from horses_performances_lead_or_beaten_distance_mappings import LEAD_OR_BEATEN_DISTANCE_MAPPINGS



class AdderDBHandler:
    # The AdderDBHandler class is intended to provide a general interface to work with the database in the process
    # of adding and consolidating race information from various source types, such as results and PP files.

    def __init__(self, db_name, table_name, include_horse=False):
        # Set up the database handler and connect to the database
        self.db = dbh.QueryDB(db_name, initialize_db=False)
        self.db.connect()
        self.table = table_name
        try:
            self.table_index = TABLE_TO_INDEX_MAPPINGS[self.table]
        except KeyError:
            print(f'Table index not found for {self.table}')
            self.table_index = 0

        # Variable to hold the db info dataframe generated by self.build_dataframe()
        self.data = None

        # Variable to control whether horse name is included in race_id construction
        self.include_horse = include_horse

    def build_dataframe(self):
        # get_race_data() returns a dataframe containing information from the target table that
        # will be aggregated into the consolidated table.

        # It first generates a dictionary with [consolidated_field_name]: [target_table_field_name]
        # as the key/value pair. It then splits these up into parallel lists of consolidated and target table
        # field names. The target table fields are used to query the db, and the consolidated field names
        # are used as column headers in the resulting data frame.

        # Because python does not guarantee the order that dictionary entries will be presented,
        # we have to extract the [consolidate]:[target] pairs together and then split them up.

        if self.table == 'horses_consolidated_races':
            source_fields = ['date', 'track', 'race_num', 'distance']
            consolidated_fields = ['date', 'track', 'race_num', 'distance']
        else:
            table_index = TABLE_TO_INDEX_MAPPINGS[self.table]
            field_dict = {key: value[table_index] for key, value in CONSOLIDATED_TABLE_STRUCTURE.items()
                          if value[table_index]}
            extra_fields = {key: value[table_index] for key, value in ADDITIONAL_FIELDS.items()
                            if value[table_index]}
            field_dict.update(extra_fields)
            fields = [(key, value) for key, value in field_dict.items()]
            source_fields = [item for _, item in fields]
            consolidated_fields = [item for item, _ in fields]

        # Query the db and return the results as a Pandas dataframe

        sql_query = self.db.generate_query(self.table, source_fields, other='LIMIT 10000')
        db_data = self.db.query_db(sql_query)
        self.data = pd.DataFrame(db_data, columns=consolidated_fields)

    def add_race_ids(self):
        # add_race_ids() is intended to generate a unique string for each row in the self.data
        # Pandas dataframe, which is used as an index for the dataframe.

        # Zip up date, track, and race_num for races in the df being processed.
        # For horses_consolidated_data, don't include a horse name (because there isn't one in that table)
        race_id_data = zip(self.data['date'], self.data['track'], self.data['race_num'], self.data['horse_name']) \
            if self.include_horse else zip(self.data['date'], self.data['track'], self.data['race_num'])

        print(f'Adding race ids to {self.table}')

        # Concatenate the zipped fields, add them as a column to the df, and set that as the df's index
        if self.include_horse:
            race_ids = [str(item[0]) + str(item[1]) + str(item[2]) + str(item[3]).upper() for item in race_id_data]
        else:
            race_ids = [str(item[0]) + str(item[1]) + str(item[2]) for item in race_id_data]

        self.data['race_id'] = race_ids
        self.data.set_index('race_id', inplace=True)

    def get_race_data(self):
        self.build_dataframe()
        self.add_race_ids()

    def get_trimmed_row_data(self, i, distance):
        # get_trimmed_row_data() returns a single row of race data from the dataframe with unused columns dropped,
        # and with columns renamed with field names appropriate for entry into the consolidated db.

        def generate_drop_cols():
            # generate_drop_cols() returns a list of the columns that need to be dropped from a
            # row of data based on the distance of the race represented by that row.
            # The domain of columns that could be dropped is ADDITIONAL FIELDS, which contains
            # fields that are specific to the source-data format and will not necessarily
            # be common to all data sources.
            #
            # The need for this process comes is driven by the fact that the source data provides
            # times and lengths based on the call rather than the distance of that call. In order to aggregate data
            # that provides that info at different calls, we need to translate those calls into distances and then
            # use those distances when aggregating the data.

            additional_cols = [key for key in ADDITIONAL_FIELDS if ADDITIONAL_FIELDS[key][self.table_index] is not None]
            position_cols = [key for key in get_position_mapping().keys()]
            margin_cols = [key for key in get_margin_mapping().keys()]
            drop_columns = [col for col in additional_cols if col not in position_cols and col not in margin_cols]

            return drop_columns

        def get_position_mapping():
            mapping = {value[self.table_index]: key for key, value in POSITION_DISTANCE_MAPPINGS[distance].items()}
            return mapping

        def get_margin_mapping():
            mapping = {value[self.table_index]: key for key, value in LEAD_OR_BEATEN_DISTANCE_MAPPINGS[distance].items()}
            return mapping

        # Get the list of columns that need to be dropped from the row
        drop_columns = generate_drop_cols()

        # Pull row data and set appropriate position/margin column names for distance
        row_data = self.data.iloc[i]
        row_data = row_data.rename(get_position_mapping())
        row_data = row_data.rename(get_margin_mapping())

        # Drop unused columns that conflict with consolidated table schema
        row_data = row_data.drop(drop_columns)

        # return the row data and the columns
        return row_data

    def fields_blank(self,  race_id, fields, number='all'):
        data = self.data.loc[race_id, fields]
        missing_items = [item == None or                                                # (1)   Value is none or it's
                         (type(item) != str and not isinstance(item, datetime.date)     # (2)(a)not a string or date
                          and np.isnan(item))                                           #    (b)and it's a nan
                         for item in data]
        if number == 'all':
            return True if all(missing_items) else False
        else:
            return True if any(missing_items) else False

    def add_blank_entry(self, *race_id, include_horse=False):
        # add_blank_entry() adds a new blank entry into the consolidated_pp_db with barebones information:
        # the race_id: the date, the track, the race number, and the horse name.
        # Additional information will be added to the entry by other methods, but this will provide
        # a base point and a race_id to use for the WHERE portion of the SQL query.

        columns = ['date', 'track', 'race_num', 'horse_name'] if include_horse else ['date', 'track', 'race_num']
        self.db.add_to_table(self.table, [*race_id], columns )

    def update_race_values(self, fields, values, race_id):
        sql = self.db.generate_update_query(self.table, fields, values, where=race_id, print_query=True)
        self.db.update_db(sql)


class RaceProcessor:
    # The PPProcessor class is intended to process the Past Performance data and aggregate it into
    # the consolidated performances table. It's primary method is add_to_consolidated_data().

    def __init__(self, db_handler, db_consolidated_handler, db_consolidated_races_handler, include_horse=False):
        self.table = db_handler.table
        self.table_index = TABLE_TO_INDEX_MAPPINGS[self.table]
        self.db = db_handler
        self.consolidated_pp_db = db_consolidated_handler
        self.consolidated_races_db = db_consolidated_races_handler
        db_handler.get_race_data()

        # Variable to control whether horse information is part of the source table
        self.include_horse = include_horse

        # State-tracking variables
        current_race_id = None
        current_date = None
        current_track = None
        current_race_num = None
        current_horse = None
        current_distance = None

    def add_to_consolidated_data(self):
        # Setup progress bar
        print("Consolidating data")
        bar = Bar('Processing existing data', max=len(self.db.data))

        # Loop through each row of dataframe and process that race info
        for i in range(len(self.db.data)):
            # Advance progress bar
            bar.next()
            # Set state with current race information
            self.set_current_info(i)

            # Check that race_distance is in distances_to_process list; if not, skip the entry
            ###############
            # NEED TO DO SOMETHING BETTER ABOUT THIS DISTANCE CHECKING; IT REQUIRES THAT
            # THE CONSOLIDATED TABLE BE SEEDED WITH A TABLE CONTAINING DISTANCE INFO--TIGHT COUPLING
            #

            try:
                distance = self.consolidated_races_db.data.loc[self.get_current_race_id(), 'distance']
            except KeyError:
                print(f'No race distance info found for {self.current_race_id}')
                continue
            if distance not in DISTANCES_TO_PROCESS: continue

            # Use the dataHandler to pull the race data for the current race and generate column list
            row_data = self.db.get_trimmed_row_data(i, distance)
            columns = row_data.index.tolist()


            try:    # Check if there is an entry in the consolidated db for this race; if not, add it.
                self.consolidated_pp_db.data.loc[self.get_current_race_id(include_horse=self.include_horse)]       # This will throw an exception if there is
                                                                            # no entry in the consolidated table. The
                                                                            # race info is added by the exception
                                                                            # handler.

                # If we've gotten this far, there is an entry; check if it's blank and add all our data if so.
                if self.consolidated_pp_db.fields_blank(self.current_race_id, columns, number='all'):
                    # Add all our data
                    pass
                else:
                    # Resolve partial data
                    pass


                # If there's an entry that already has data in it, compare each data entry, see where
                # discrepancies are, resolve them, and then update the consolidated db entry.

            except KeyError:    # Add the race if there isn't already an entry in the consolidated db
                self.consolidated_pp_db.add_blank_entry(self.get_current_race_id(as_tuple=True, include_horse=self.include_horse),
                                        include_horse=self.include_horse)
                self.consolidated_pp_db.update_race_values(columns,
                                           row_data.tolist(),
                                           self.get_current_race_id(as_sql=True, include_horse=self.include_horse))

    def set_current_info(self, i):

        date_col = self.db.data.columns.get_loc('date')
        track_col = self.db.data.columns.get_loc('track')
        race_num_col = self.db.data.columns.get_loc('race_num')
        horse_name_col = self.db.data.columns.get_loc('horse_name')

        self.current_date = self.db.data.iloc[i, date_col]
        self.current_track = self.db.data.iloc[i, track_col]
        self.current_race_num = self.db.data.iloc[i, race_num_col]
        self.current_horse = self.db.data.iloc[i, horse_name_col]

        self.current_race_id = self.get_current_race_id(include_horse=self.include_horse)

    def get_current_race_id(self, include_horse=False, as_sql=False, as_tuple=False):
        if as_sql:
            race_id = f'track="{self.current_track}" ' \
                      f'AND date="{self.current_date}" ' \
                      f'AND race_num="{self.current_race_num}"'
            if include_horse: race_id += f' AND horse_name="{self.current_horse}"'
        elif as_tuple:
            if include_horse:
                race_id= (str(self.current_date), str(self.current_track), str(self.current_race_num), str(self.current_horse))
            else:
                race_id = (str(self.current_date), str(self.current_track), str(self.current_race_num))
        else:
            race_id = str(self.current_date) + str(self.current_track) + str(self.current_race_num)
            if include_horse: race_id += str(self.current_horse).upper()
        return race_id



